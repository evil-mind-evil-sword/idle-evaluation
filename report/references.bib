@article{meyerson2025maker,
  title={Solving a Million-Step LLM Task with Zero Errors},
  author={Meyerson, Elliot and Paolo, Giuseppe and Dailey, Roberto and Shahrzad, Hormoz and Francon, Olivier and Hayes, Conor F. and Qiu, Xin and Hodjat, Babak and Miikkulainen, Risto},
  journal={arXiv preprint arXiv:2511.09030},
  year={2025},
  note={MAKER framework for million-step zero-error LLM tasks via microagent decomposition}
}

@article{du2023debate,
  title={Improving Factuality and Reasoning in Language Models through Multiagent Debate},
  author={Du, Yilun and Li, Shuang and Torralba, Antonio and Tenenbaum, Joshua B. and Mordatch, Igor},
  journal={arXiv preprint arXiv:2305.14325},
  year={2023},
  note={Multi-agent debate improves LLM accuracy}
}

@article{liang2023encouraging,
  title={Encouraging Divergent Thinking in Large Language Models through Multi-Agent Debate},
  author={Liang, Tian and He, Zhiwei and Jiao, Wenxiang and Wang, Xing and Wang, Yan and Wang, Rui and Yang, Yujiu and Tu, Zhaopeng and Shi, Shuming},
  journal={arXiv preprint arXiv:2305.19118},
  year={2023}
}

@article{huang2023selfverification,
  title={Large Language Models Cannot Self-Correct Reasoning Yet},
  author={Huang, Jie and Gu, Shikang and Hou, Le and Wu, Yuexin and Wang, Xuezhi and Yu, Hongkun and Han, Jiawei},
  journal={arXiv preprint arXiv:2310.01798},
  year={2023},
  note={Shows LLMs struggle to verify their own outputs}
}

@inproceedings{jimenez2023swebench,
  title={SWE-bench: Can Language Models Resolve Real-World GitHub Issues?},
  author={Jimenez, Carlos E. and Yang, John and Wettig, Alexander and Yao, Shunyu and Pei, Kexin and Press, Ofir and Narasimhan, Karthik},
  booktitle={ICLR},
  year={2024},
  note={Real-world software engineering benchmark from GitHub issues}
}

@article{sierra2024taubench,
  title={Ï„-Bench: A Benchmark for Tool-Agent-User Interaction in Real-World Domains},
  author={Sierra AI},
  year={2024},
  note={Long-horizon conversational benchmark with pass^k reliability metric}
}

@article{chen2021evaluating,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021},
  note={Introduces pass@k metric for code generation}
}

@misc{terminalbench2025,
  title={Terminal-Bench: Evaluating AI Agents in Command-Line Environments},
  author={Stanford University and Laude Institute},
  year={2025},
  note={Multi-step CLI workflow benchmark}
}

@misc{contextbench2024,
  title={Context-Bench: Evaluating Long-Context Reasoning in AI Agents},
  author={Letta},
  year={2024},
  note={Long-context management benchmark}
}

@article{yang2024swebench,
  title={SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering},
  author={Yang, John and Jimenez, Carlos E. and Wettig, Alexander and Liber, Kilian and Yao, Shunyu and Narasimhan, Karthik and Press, Ofir},
  journal={arXiv preprint arXiv:2405.15793},
  year={2024}
}
